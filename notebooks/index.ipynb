{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluxo Geral\n",
    "1. Carregamento e Exploração dos Dados\n",
    "2. Pré-Processamento dos Dados\n",
    "3. Divisão dos Dados (Treino, Validação e Teste)\n",
    "4. Treinamento de Modelos\n",
    "5. Avaliação de Modelos\n",
    "6. Ajustes e Melhorias\n",
    "7. Implantação da IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Carregamento e Exploração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../data/raw/dataset_transacoes.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Visualizar as primeiras linhas\n",
    "print(df.head())\n",
    "\n",
    "# Informações gerais sobre os dados\n",
    "print(df.info())\n",
    "\n",
    "# Resumo estatístico\n",
    "print(df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Análise Exploratória\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição das Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x=\"classe\", data=df)\n",
    "plt.title(\"Distribuição das Classes (Fraude vs Não Fraude)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlação entre Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas categóricas\n",
    "categorical_cols = [\"localizacao\", \"tipo_estabelecimento\", \"dia_semana\", \"categoria_despesa\", \"genero_titular\", \"historico_pagamento\"]\n",
    "\n",
    "# Codificar variáveis categóricas usando One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# Gerar a matriz de correlação para o DataFrame codificado\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_encoded.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Matriz de Correlação (Atributos Codificados)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar apenas colunas numéricas\n",
    "numerical_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# Gerar a matriz de correlação\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numerical_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Matriz de Correlação (Atributos Numéricos)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizar Correlações Apenas para Colunas de Interesse\n",
    "\n",
    "Se você quer ver correlações apenas entre algumas colunas específicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selecionar colunas específicas\n",
    "cols_of_interest = [\"valor_transacao\", \"limite_credito\", \"saldo_atual\", \"classe\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[cols_of_interest].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlação Entre Colunas Selecionadas\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização de Dados Numéricos\n",
    "Adicionaremos a normalização para as colunas numéricas, garantindo que todos os atributos estejam na mesma escala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Selecionar colunas numéricas para normalização\n",
    "numerical_cols = [\n",
    "    \"valor_transacao\", \"limite_credito\", \"saldo_atual\", \"valor_medio_transacoes\",\n",
    "    \"utilizacao_credito\", \"tempo_desde_ultima_transacao\"\n",
    "]\n",
    "\n",
    "# Criar o escalador\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizar os dados numéricos\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "# Verificar as primeiras linhas após a normalização\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do Dataset\n",
    "Dividiremos os dados em conjuntos de treino, validação e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separar atributos (X) e classe (y)\n",
    "X = df_encoded.drop(\"classe\", axis=1)\n",
    "y = df_encoded[\"classe\"]\n",
    "\n",
    "# Divisão inicial: 70% treino, 30% temporário\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Divisão secundária: 15% validação, 15% teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Confirmar os tamanhos dos conjuntos\n",
    "print(f\"Tamanho do conjunto de treino: {X_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de validação: {X_val.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento Inicial com Random Forest\n",
    "Usaremos a Random Forest como primeiro modelo para classificar as transações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Criar o modelo\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Relatório de Classificação:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"Matriz de Confusão:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise do Problema\n",
    "### Matriz de Confusão:\n",
    "\n",
    "O modelo previu corretamente todas as instâncias da classe 0 (144), mas não conseguiu prever nenhuma instância da classe 1 (todas foram classificadas como 0).\n",
    "### Métricas:\n",
    "\n",
    "- Precision (1): Zero porque o modelo não fez nenhuma previsão correta da classe 1.\n",
    "- Recall (1): Zero porque o modelo não conseguiu identificar nenhuma instância real da classe 1.\n",
    "Causa:\n",
    "\n",
    "O dataset está altamente desbalanceado (cerca de 5% de fraudes). Modelos de aprendizado de máquina padrão tendem a favorecer a classe majoritária.\n",
    "\n",
    "## Soluções\n",
    "Vamos abordar o problema de desbalanceamento com algumas técnicas:\n",
    "\n",
    "1. Reamostragem do Dataset\n",
    "Oversampling (Aumentar a classe minoritária):\n",
    "Usaremos SMOTE para gerar amostras sintéticas da classe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Criar o objeto SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Aplicar oversampling nos dados de treino\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verificar a nova distribuição\n",
    "print(\"Distribuição após SMOTE:\")\n",
    "print(y_train_balanced.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o modelo Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Treinar o modelo com os dados balanceados\n",
    "model_balanced = RandomForestClassifier(random_state=42)\n",
    "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred_balanced = model_balanced.predict(X_val)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Relatório de Classificação (Dados Balanceados):\\n\", classification_report(y_val, y_pred_balanced))\n",
    "print(\"Matriz de Confusão (Dados Balanceados):\\n\", confusion_matrix(y_val, y_pred_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE também no conjunto de validação\n",
    "X_val_balanced, y_val_balanced = smote.fit_resample(X_val, y_val)\n",
    "\n",
    "# Reavaliar o modelo com os dados balanceados no conjunto de validação\n",
    "y_pred_balanced_val = model_balanced.predict(X_val_balanced)\n",
    "\n",
    "# Avaliar o modelo novamente\n",
    "print(\"Relatório de Classificação (Validação Balanceada):\\n\", classification_report(y_val_balanced, y_pred_balanced_val))\n",
    "print(\"Matriz de Confusão (Validação Balanceada):\\n\", confusion_matrix(y_val_balanced, y_pred_balanced_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Criar o modelo XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42, scale_pos_weight=len(y_train_balanced) / sum(y_train_balanced == 1))\n",
    "\n",
    "# Treinar o modelo com os dados balanceados\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(\"Relatório de Classificação (XGBoost):\\n\", classification_report(y_val, y_pred_xgb))\n",
    "print(\"Matriz de Confusão (XGBoost):\\n\", confusion_matrix(y_val, y_pred_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
